#!/bin/bash

# Setup script for Rust ML project
# This script helps you configure your environment files

set -e  # Exit on error

echo "╔════════════════════════════════════════════════════════════════╗"
echo "║                                                                ║"
echo "║     🦀 RUST ML v2.0 - Environment Setup Helper 🦀              ║"
echo "║                                                                ║"
echo "╚════════════════════════════════════════════════════════════════╝"
echo ""

# ============================================================================
# Step 1: Check if PyTorch is installed
# ============================================================================

echo "📋 Step 1: Checking PyTorch installation..."

if ! command -v python3 &> /dev/null; then
    echo "❌ Python3 not found. Please install Python3 first."
    exit 1
fi

if ! python3 -c "import torch" 2>/dev/null; then
    echo "❌ PyTorch not found. Installing PyTorch with CUDA support..."
    echo ""
    echo "Run one of these commands:"
    echo "  pip install torch --index-url https://download.pytorch.org/whl/cu121"
    echo "  pip install torch --index-url https://download.pytorch.org/whl/cu118"
    exit 1
fi

echo "✅ PyTorch is installed"

# ============================================================================
# Step 2: Detect PyTorch paths
# ============================================================================

echo ""
echo "📋 Step 2: Detecting PyTorch installation paths..."

TORCH_PATH=$(python3 -c "import torch; print(torch.__path__[0])" 2>/dev/null)

if [ -z "$TORCH_PATH" ]; then
    echo "❌ Could not detect PyTorch path"
    exit 1
fi

TORCH_LIB="${TORCH_PATH}/lib"
NVIDIA_BASE=$(dirname "$TORCH_PATH")/nvidia

echo "✅ Detected paths:"
echo "   PyTorch:    $TORCH_PATH"
echo "   Torch lib:  $TORCH_LIB"
echo "   NVIDIA:     $NVIDIA_BASE"

# ============================================================================
# Step 3: Check CUDA availability
# ============================================================================

echo ""
echo "📋 Step 3: Checking CUDA availability..."

CUDA_AVAILABLE=$(python3 -c "import torch; print(torch.cuda.is_available())" 2>/dev/null)

if [ "$CUDA_AVAILABLE" = "True" ]; then
    echo "✅ CUDA is available in PyTorch"
    CUDA_VERSION=$(python3 -c "import torch; print(torch.version.cuda)" 2>/dev/null)
    DEVICE_NAME=$(python3 -c "import torch; print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A')" 2>/dev/null)
    echo "   CUDA version: $CUDA_VERSION"
    echo "   GPU: $DEVICE_NAME"
else
    echo "⚠️  CUDA not available in PyTorch"
    echo "   Project will run on CPU (slower)"
fi

# ============================================================================
# Step 4: Create environment files
# ============================================================================

echo ""
echo "📋 Step 4: Creating environment files..."

# Create cuda_env.sh
if [ -f "cuda_env.sh" ]; then
    echo "⚠️  cuda_env.sh already exists. Creating backup..."
    cp cuda_env.sh cuda_env.sh.backup
fi

cat > cuda_env.sh << EOF
# CUDA environment setup for Rust+PyTorch
# Auto-generated by setup.sh
# Source this file before building or running: source cuda_env.sh

TORCH_LIB="$TORCH_LIB"
NVIDIA_BASE="$NVIDIA_BASE"

export LIBTORCH="$TORCH_PATH"
export LIBTORCH_USE_PYTORCH=1

export LD_LIBRARY_PATH="\${TORCH_LIB}:\${NVIDIA_BASE}/cuda_runtime/lib:\${NVIDIA_BASE}/cublas/lib:\${NVIDIA_BASE}/cudnn/lib:\${NVIDIA_BASE}/cufft/lib:\${NVIDIA_BASE}/curand/lib:\${NVIDIA_BASE}/cusolver/lib:\${NVIDIA_BASE}/cusparse/lib:\${NVIDIA_BASE}/cuda_nvrtc/lib:\${NVIDIA_BASE}/cuda_cupti/lib:\$LD_LIBRARY_PATH"

export LD_PRELOAD="\${TORCH_LIB}/libtorch_cuda.so:\${TORCH_LIB}/libc10_cuda.so"

echo "✓ CUDA environment configured"
echo "  LIBTORCH: \$LIBTORCH"
echo "  Now you can run: cargo run --release"
EOF

echo "✅ Created cuda_env.sh"

# Create run_cuda.sh
if [ -f "run_cuda.sh" ]; then
    echo "⚠️  run_cuda.sh already exists. Creating backup..."
    cp run_cuda.sh run_cuda.sh.backup
fi

cat > run_cuda.sh << EOF
#!/bin/bash

# Python PyTorch CUDA kütüphanelerinin yolu
TORCH_LIB="$TORCH_LIB"
NVIDIA_BASE="$NVIDIA_BASE"

# Tüm NVIDIA CUDA kütüphanelerini ekle
export LD_LIBRARY_PATH="\${TORCH_LIB}:\${NVIDIA_BASE}/cuda_runtime/lib:\${NVIDIA_BASE}/cublas/lib:\${NVIDIA_BASE}/cudnn/lib:\${NVIDIA_BASE}/cufft/lib:\${NVIDIA_BASE}/curand/lib:\${NVIDIA_BASE}/cusolver/lib:\${NVIDIA_BASE}/cusparse/lib:\${NVIDIA_BASE}/cuda_nvrtc/lib:\${NVIDIA_BASE}/cuda_cupti/lib:\$LD_LIBRARY_PATH"

# libtorch_cuda.so'nun yüklenmesini zorla
export LD_PRELOAD="\${TORCH_LIB}/libtorch_cuda.so:\${TORCH_LIB}/libc10_cuda.so"

echo "LD_LIBRARY_PATH set"
echo "LD_PRELOAD set to force load CUDA libraries"
echo ""

./target/release/rust-ml
EOF

chmod +x run_cuda.sh
echo "✅ Created run_cuda.sh (executable)"

# ============================================================================
# Step 5: Build project
# ============================================================================

echo ""
echo "📋 Step 5: Building project..."
echo ""

source cuda_env.sh
cargo build --release

if [ $? -eq 0 ]; then
    echo ""
    echo "✅ Build successful!"
else
    echo ""
    echo "❌ Build failed. Please check the errors above."
    exit 1
fi

# ============================================================================
# Summary
# ============================================================================

echo ""
echo "╔════════════════════════════════════════════════════════════════╗"
echo "║                    ✅ Setup Complete! ✅                        ║"
echo "╚════════════════════════════════════════════════════════════════╝"
echo ""
echo "📁 Created files:"
echo "   ✓ cuda_env.sh"
echo "   ✓ run_cuda.sh"
echo ""
echo "🚀 To run the project:"
echo "   ./run_cuda.sh"
echo ""
echo "Or manually:"
echo "   source cuda_env.sh"
echo "   cargo run --release"
echo ""
echo "🎯 The project will run 13 AI/ML techniques in ~2 seconds!"
echo ""
